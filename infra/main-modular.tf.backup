# MLOps Taxi Prediction Infrastructure - Modular Configuration
# This version uses modules for better organization and reusability

terraform {
  required_version = ">= 1.0"
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
  
  backend "s3" {
    bucket         = "mlops-taxi-prediction-terraform-state"
    key            = "terraform.tfstate"
    region         = "eu-north-1"
    encrypt        = true
    dynamodb_table = "terraform-state-lock"
  }
}

provider "aws" {
  region = var.aws_region
}

# VPC Module
module "vpc" {
  source = "./modules/vpc"
  
  project_name            = var.project_name
  environment            = var.environment
  vpc_cidr               = var.vpc_cidr
  public_subnet_cidrs    = var.public_subnet_cidrs
  private_subnet_cidrs   = var.private_subnet_cidrs
  enable_nat_gateway     = var.enable_nat_gateway
  single_nat_gateway     = var.single_nat_gateway
  enable_vpc_flow_logs   = var.enable_vpc_flow_logs
  enable_s3_endpoint     = var.enable_s3_endpoint
  
  tags = var.tags
}

# Security Groups Module
module "security_groups" {
  source = "./modules/security-groups"
  
  project_name               = var.project_name
  environment               = var.environment
  vpc_id                    = module.vpc.vpc_id
  vpc_cidr                  = module.vpc.vpc_cidr_block
  
  # Optional security groups
  create_lambda_sg          = true
  create_vpc_endpoints_sg   = var.enable_s3_endpoint
  
  # Production security configurations
  alb_allowed_cidrs         = var.environment == "prod" ? var.production_allowed_cidrs : ["0.0.0.0/0"]
  enable_ssh_access         = var.environment != "prod"  # Disable SSH in production
  
  tags = var.tags
}

# ECR Module
module "ecr" {
  source = "./modules/ecr"
  
  project_name             = var.project_name
  environment             = var.environment
  enable_image_scanning   = var.ecr_enable_image_scanning
  max_image_count         = var.ecr_max_image_count
  untagged_expiry_days    = var.ecr_untagged_expiry_days
  
  tags = var.tags
}

# S3 Buckets
resource "aws_s3_bucket" "mlflow_artifacts" {
  bucket = "${var.project_name}-mlflow-artifacts-${var.environment}"
  tags   = var.tags
}

resource "aws_s3_bucket" "data_storage" {
  bucket = "${var.project_name}-data-storage-${var.environment}"
  tags   = var.tags
}

resource "aws_s3_bucket" "monitoring_reports" {
  bucket = "${var.project_name}-monitoring-reports-${var.environment}"
  tags   = var.tags
}

# S3 Bucket versioning
resource "aws_s3_bucket_versioning" "mlflow_artifacts_versioning" {
  bucket = aws_s3_bucket.mlflow_artifacts.id
  versioning_configuration {
    status = "Enabled"
  }
}

resource "aws_s3_bucket_versioning" "data_storage_versioning" {
  bucket = aws_s3_bucket.data_storage.id
  versioning_configuration {
    status = "Enabled"
  }
}

# S3 Bucket server-side encryption
resource "aws_s3_bucket_server_side_encryption_configuration" "mlflow_artifacts_encryption" {
  bucket = aws_s3_bucket.mlflow_artifacts.id

  rule {
    apply_server_side_encryption_by_default {
      sse_algorithm = "AES256"
    }
  }
}

resource "aws_s3_bucket_server_side_encryption_configuration" "data_storage_encryption" {
  bucket = aws_s3_bucket.data_storage.id

  rule {
    apply_server_side_encryption_by_default {
      sse_algorithm = "AES256"
    }
  }
}

resource "aws_s3_bucket_server_side_encryption_configuration" "monitoring_reports_encryption" {
  bucket = aws_s3_bucket.monitoring_reports.id

  rule {
    apply_server_side_encryption_by_default {
      sse_algorithm = "AES256"
    }
  }
}

# Block public access for all buckets
resource "aws_s3_bucket_public_access_block" "mlflow_artifacts_pab" {
  bucket = aws_s3_bucket.mlflow_artifacts.id

  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true
}

resource "aws_s3_bucket_public_access_block" "data_storage_pab" {
  bucket = aws_s3_bucket.data_storage.id

  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true
}

resource "aws_s3_bucket_public_access_block" "monitoring_reports_pab" {
  bucket = aws_s3_bucket.monitoring_reports.id

  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true
}

# Kinesis Data Stream
resource "aws_kinesis_stream" "taxi_predictions" {
  name             = "taxi-ride-predictions-stream"
  shard_count      = var.kinesis_shard_count
  retention_period = 24

  shard_level_metrics = [
    "IncomingRecords",
    "OutgoingRecords",
  ]

  tags = var.tags
}

# IAM Role for Lambda
resource "aws_iam_role" "lambda_role" {
  name = "${var.project_name}-lambda-role-${var.environment}"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Action = "sts:AssumeRole"
        Effect = "Allow"
        Principal = {
          Service = "lambda.amazonaws.com"
        }
      }
    ]
  })

  tags = var.tags
}

# IAM Policy for Lambda
resource "aws_iam_policy" "lambda_policy" {
  name        = "${var.project_name}-lambda-policy-${var.environment}"
  description = "IAM policy for Lambda function with least privilege access"

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Action = [
          "kinesis:GetRecords",
          "kinesis:GetShardIterator",
          "kinesis:DescribeStream",
          "kinesis:ListStreams"
        ]
        Resource = aws_kinesis_stream.taxi_predictions.arn
      },
      {
        Effect = "Allow"
        Action = [
          "s3:GetObject"
        ]
        Resource = "${aws_s3_bucket.data_storage.arn}/*"
      },
      {
        Effect = "Allow"
        Action = [
          "s3:PutObject"
        ]
        Resource = "${aws_s3_bucket.data_storage.arn}/*"
      },
      {
        Effect = "Allow"
        Action = [
          "logs:CreateLogGroup",
          "logs:CreateLogStream",
          "logs:PutLogEvents"
        ]
        Resource = "arn:aws:logs:*:*:*"
      }
    ]
  })

  tags = var.tags
}

# Attach policy to role
resource "aws_iam_role_policy_attachment" "lambda_policy_attachment" {
  role       = aws_iam_role.lambda_role.name
  policy_arn = aws_iam_policy.lambda_policy.arn
}

# Lambda function
# resource "aws_lambda_function" "taxi_predictor" {
#   filename         = "../taxi_predictor.zip"
#   function_name    = "taxi-trip-duration-predictor"
#   role            = aws_iam_role.lambda_role.arn
#   handler         = "lambda_function.lambda_handler"
#   source_code_hash = filebase64sha256("../taxi_predictor.zip")
#   runtime         = var.lambda_runtime
#   memory_size     = var.lambda_memory_size
#   timeout         = var.lambda_timeout
# 
#   environment {
#     variables = {
#       DATA_STORAGE_BUCKET = aws_s3_bucket.data_storage.bucket
#       MODEL_S3_KEY = "models/latest/combined_model.pkl"
#       PREDICTIONS_S3_BUCKET = aws_s3_bucket.data_storage.bucket
#     }
#   }
# 
#   tags = var.tags
# 
#   depends_on = [
#     aws_iam_role_policy_attachment.lambda_policy_attachment,
#   ]
# }

# Lambda event source mapping for Kinesis - commented out until Lambda function is available
# resource "aws_lambda_event_source_mapping" "kinesis_lambda_mapping" {
#   event_source_arn  = aws_kinesis_stream.taxi_predictions.arn
#   function_name     = aws_lambda_function.taxi_predictor.arn
#   starting_position = "LATEST"
#   batch_size        = 10
#
#   depends_on = [aws_iam_role_policy_attachment.lambda_policy_attachment]
# }

# CloudWatch Log Group for Lambda - commented out until Lambda function is available
# resource "aws_cloudwatch_log_group" "lambda_logs" {
#   name              = "/aws/lambda/${aws_lambda_function.taxi_predictor.function_name}"
#   retention_in_days = 14
#
#   tags = var.tags
# }

# RDS Subnet Group
resource "aws_db_subnet_group" "main" {
  name       = "${var.project_name}-db-subnet-group-${var.environment}"
  subnet_ids = module.vpc.private_subnet_ids

  tags = merge(var.tags, {
    Name = "${var.project_name}-db-subnet-group-${var.environment}"
  })
}

# RDS Instance for Airflow
resource "aws_db_instance" "airflow" {
  identifier     = "${var.project_name}-airflow-db-${var.environment}"
  engine         = "postgres"
  engine_version = var.rds_postgres_version
  instance_class = var.rds_instance_class
  
  allocated_storage     = var.rds_allocated_storage
  max_allocated_storage = var.rds_max_allocated_storage
  storage_type          = "gp3"
  storage_encrypted     = true
  
  db_name  = var.airflow_db_name
  username = var.airflow_db_username
  password = var.airflow_db_password
  
  vpc_security_group_ids = [module.security_groups.rds_security_group_id]
  db_subnet_group_name   = aws_db_subnet_group.main.name
  
  backup_retention_period = 7
  backup_window          = "03:00-04:00"
  maintenance_window     = "Sun:04:00-Sun:05:00"
  
  skip_final_snapshot = var.environment == "dev" ? true : false
  deletion_protection = var.environment == "prod" ? true : false
  
  performance_insights_enabled = var.rds_performance_insights_enabled
  monitoring_interval         = var.rds_monitoring_interval
  
  tags = merge(var.tags, {
    Name = "${var.project_name}-airflow-db-${var.environment}"
  })
}

# EFS File System
resource "aws_efs_file_system" "main" {
  creation_token = "${var.project_name}-efs-${var.environment}"
  
  performance_mode = "generalPurpose"
  throughput_mode  = var.efs_provisioned_throughput > 0 ? "provisioned" : "bursting"
  
  # Only set provisioned throughput if > 0 (for bursting mode, this is not needed)
  provisioned_throughput_in_mibps = var.efs_provisioned_throughput > 0 ? var.efs_provisioned_throughput : null
  
  encrypted = true
  
  lifecycle_policy {
    transition_to_ia = "AFTER_30_DAYS"
  }
  
  lifecycle_policy {
    transition_to_primary_storage_class = "AFTER_1_ACCESS"
  }

  tags = merge(var.tags, {
    Name = "${var.project_name}-efs-${var.environment}"
  })
}

# EFS Mount Targets
resource "aws_efs_mount_target" "main" {
  count = length(module.vpc.private_subnet_ids)
  
  file_system_id  = aws_efs_file_system.main.id
  subnet_id       = module.vpc.private_subnet_ids[count.index]
  security_groups = [module.security_groups.efs_security_group_id]
}

# ECS Cluster
resource "aws_ecs_cluster" "main" {
  name = "${var.project_name}-cluster-${var.environment}"
  
  configuration {
    execute_command_configuration {
      logging = "OVERRIDE"
      
      log_configuration {
        cloud_watch_log_group_name = aws_cloudwatch_log_group.ecs_cluster.name
      }
    }
  }

  tags = var.tags
}

# ECS Cluster Capacity Providers
resource "aws_ecs_cluster_capacity_providers" "main" {
  cluster_name = aws_ecs_cluster.main.name
  
  capacity_providers = ["FARGATE", "FARGATE_SPOT"]
  
  default_capacity_provider_strategy {
    base              = 1
    weight            = 100
    capacity_provider = "FARGATE"
  }
}

# CloudWatch Log Group for ECS Cluster
resource "aws_cloudwatch_log_group" "ecs_cluster" {
  name              = "/aws/ecs/cluster/${var.project_name}-${var.environment}"
  retention_in_days = 14
  
  tags = var.tags
}

# CloudWatch Log Group for Airflow
resource "aws_cloudwatch_log_group" "airflow" {
  name              = "/ecs/${var.project_name}-airflow-${var.environment}"
  retention_in_days = 14
  
  tags = var.tags
}

# CloudWatch Log Group for MLflow
resource "aws_cloudwatch_log_group" "mlflow" {
  name              = "/ecs/${var.project_name}-mlflow-${var.environment}"
  retention_in_days = 14
  
  tags = var.tags
}

# ECS Task Definition for Airflow
resource "aws_ecs_task_definition" "airflow" {
  family                   = "${var.project_name}-airflow-${var.environment}"
  network_mode             = "awsvpc"
  requires_compatibilities = ["FARGATE"]
  cpu                      = var.airflow_cpu
  memory                   = var.airflow_memory
  execution_role_arn       = aws_iam_role.ecs_task_execution.arn
  task_role_arn           = aws_iam_role.ecs_task.arn

  container_definitions = jsonencode([
    {
      name  = "airflow-webserver"
      image = "${module.ecr.airflow_repository_url}:latest"
      
      portMappings = [
        {
          containerPort = 8080
          hostPort      = 8080
          protocol      = "tcp"
        }
      ]
      
      essential = true
      command = ["airflow", "api-server"]
      
      environment = [
        {
          name  = "AIRFLOW__CORE__EXECUTOR"
          value = "LocalExecutor"
        },
        {
          name  = "AIRFLOW__DATABASE__SQL_ALCHEMY_CONN"
          value = "postgresql+psycopg2://${var.airflow_db_username}:${var.airflow_db_password}@${aws_db_instance.airflow.endpoint}/${var.airflow_db_name}"
        },
        {
          name  = "AIRFLOW__CORE__FERNET_KEY"
          value = var.airflow_fernet_key
        },
        {
          name  = "AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION"
          value = "true"
        },
        {
          name  = "AIRFLOW__CORE__LOAD_EXAMPLES"
          value = "false"
        },
        {
          name  = "AIRFLOW__API__AUTH_BACKENDS"
          value = "airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session"
        },
        {
          name  = "AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK"
          value = "true"
        },
        {
          name  = "AWS_DEFAULT_REGION"
          value = var.aws_region
        },
        {
          name  = "DATA_STORAGE_BUCKET"
          value = aws_s3_bucket.data_storage.bucket
        },
        {
          name  = "KINESIS_STREAM_NAME"
          value = aws_kinesis_stream.taxi_predictions.name
        },
        {
          name  = "MLFLOW_TRACKING_URI"
          value = "http://mlflow.${var.project_name}-${var.environment}.local:5000"
        },
        {
          name  = "MLFLOW_BUCKET_NAME"
          value = aws_s3_bucket.mlflow_artifacts.bucket
        },
        {
          name  = "MONITORING_BUCKET_NAME"
          value = aws_s3_bucket.monitoring_reports.bucket
        }
      ]
      
      logConfiguration = {
        logDriver = "awslogs"
        options = {
          "awslogs-group"         = aws_cloudwatch_log_group.airflow.name
          "awslogs-region"        = var.aws_region
          "awslogs-stream-prefix" = "airflow-webserver"
        }
      }
      
      mountPoints = [
        {
          sourceVolume  = "efs-storage"
          containerPath = "/opt/airflow/dags"
          readOnly      = false
        },
        {
          sourceVolume  = "efs-storage"
          containerPath = "/opt/airflow/logs"
          readOnly      = false
        }
      ]
      
      healthCheck = {
        command = ["CMD-SHELL", "curl -f http://localhost:8080/health || exit 1"]
        interval = 30
        timeout = 5
        retries = 3
        startPeriod = 60
      }
    },
    {
      name  = "airflow-scheduler"
      image = "${module.ecr.airflow_repository_url}:latest"
      
      command = ["airflow", "scheduler"]
      essential = true
      
      environment = [
        {
          name  = "AIRFLOW__CORE__EXECUTOR"
          value = "LocalExecutor"
        },
        {
          name  = "AIRFLOW__DATABASE__SQL_ALCHEMY_CONN"
          value = "postgresql+psycopg2://${var.airflow_db_username}:${var.airflow_db_password}@${aws_db_instance.airflow.endpoint}/${var.airflow_db_name}"
        },
        {
          name  = "AIRFLOW__CORE__FERNET_KEY"
          value = var.airflow_fernet_key
        },
        {
          name  = "AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION"
          value = "true"
        },
        {
          name  = "AIRFLOW__CORE__LOAD_EXAMPLES"
          value = "false"
        },
        {
          name  = "AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK"
          value = "true"
        },
        {
          name  = "AWS_DEFAULT_REGION"
          value = var.aws_region
        },
        {
          name  = "DATA_STORAGE_BUCKET"
          value = aws_s3_bucket.data_storage.bucket
        },
        {
          name  = "KINESIS_STREAM_NAME"
          value = aws_kinesis_stream.taxi_predictions.name
        },
        {
          name  = "MLFLOW_TRACKING_URI"
          value = "http://mlflow.${var.project_name}-${var.environment}.local:5000"
        },
        {
          name  = "MLFLOW_BUCKET_NAME"
          value = aws_s3_bucket.mlflow_artifacts.bucket
        },
        {
          name  = "MONITORING_BUCKET_NAME"
          value = aws_s3_bucket.monitoring_reports.bucket
        }
      ]
      
      logConfiguration = {
        logDriver = "awslogs"
        options = {
          "awslogs-group"         = aws_cloudwatch_log_group.airflow.name
          "awslogs-region"        = var.aws_region
          "awslogs-stream-prefix" = "airflow-scheduler"
        }
      }
      
      mountPoints = [
        {
          sourceVolume  = "efs-storage"
          containerPath = "/opt/airflow/dags"
          readOnly      = false
        },
        {
          sourceVolume  = "efs-storage"
          containerPath = "/opt/airflow/logs"
          readOnly      = false
        }
      ]
      
      healthCheck = {
        command = ["CMD-SHELL", "curl -f http://localhost:8974/health || exit 1"]
        interval = 30
        timeout = 5
        retries = 3
        startPeriod = 60
      }
    }
  ])

  volume {
    name = "efs-storage"
    
    efs_volume_configuration {
      file_system_id = aws_efs_file_system.main.id
      root_directory = "/"
      transit_encryption = "ENABLED"
      
      authorization_config {
        access_point_id = aws_efs_access_point.airflow.id
        iam             = "ENABLED"
      }
    }
  }

  tags = var.tags
}

# ECS Task Definition for MLflow
resource "aws_ecs_task_definition" "mlflow" {
  family                   = "${var.project_name}-mlflow-${var.environment}"
  network_mode             = "awsvpc"
  requires_compatibilities = ["FARGATE"]
  cpu                      = var.mlflow_cpu
  memory                   = var.mlflow_memory
  execution_role_arn       = aws_iam_role.ecs_task_execution.arn
  task_role_arn            = aws_iam_role.ecs_task.arn

  container_definitions = jsonencode([
    {
      name  = "mlflow-server"
      image = "${module.ecr.mlflow_repository_url}:latest"
      
      portMappings = [
        {
          containerPort = 5000
          hostPort      = 5000
          protocol      = "tcp"
        }
      ]
      
      essential = true
      
      environment = [
        {
          name  = "MLFLOW__BACKEND_STORE_URI"
          value = "postgresql+psycopg2://${var.airflow_db_username}:${var.airflow_db_password}@${aws_db_instance.airflow.endpoint}/mlflow"
        },
        {
          name  = "MLFLOW__DEFAULT_ARTIFACT_ROOT"
          value = "s3://${aws_s3_bucket.mlflow_artifacts.bucket}/artifacts"
        },
        {
          name  = "AWS_DEFAULT_REGION"
          value = var.aws_region
        },
        {
          name  = "MLFLOW_S3_ENDPOINT_URL"
          value = "https://s3.${var.aws_region}.amazonaws.com"
        }
      ]
      
      command = [
        "mlflow", "server",
        "--backend-store-uri", "postgresql+psycopg2://${var.airflow_db_username}:${var.airflow_db_password}@${aws_db_instance.airflow.endpoint}/mlflow",
        "--default-artifact-root", "s3://${aws_s3_bucket.mlflow_artifacts.bucket}/artifacts",
        "--host", "0.0.0.0",
        "--port", "5000"
      ]
      
      logConfiguration = {
        logDriver = "awslogs"
        options = {
          "awslogs-group"         = aws_cloudwatch_log_group.mlflow.name
          "awslogs-region"        = var.aws_region
          "awslogs-stream-prefix" = "mlflow-server"
        }
      }
      
      mountPoints = [
        {
          sourceVolume  = "efs-storage"
          containerPath = "/mlflow/data"
          readOnly      = false
        }
      ]
      
      healthCheck = {
        command = ["CMD-SHELL", "curl -f http://localhost:5000/health || exit 1"]
        interval = 30
        timeout = 5
        retries = 3
        startPeriod = 60
      }
    }
  ])

  volume {
    name = "efs-storage"
    
    efs_volume_configuration {
      file_system_id = aws_efs_file_system.main.id
      root_directory = "/"
      transit_encryption = "ENABLED"
      
      authorization_config {
        access_point_id = aws_efs_access_point.mlflow.id
        iam             = "ENABLED"
      }
    }
  }

  tags = var.tags
}

# EFS Access Points for proper isolation
resource "aws_efs_access_point" "airflow" {
  file_system_id = aws_efs_file_system.main.id
  
  posix_user {
    gid = 50000
    uid = 50000
  }
  
  root_directory {
    path = "/airflow"
    creation_info {
      owner_gid   = 50000
      owner_uid   = 50000
      permissions = "0755"
    }
  }
  
  tags = merge(var.tags, {
    Name = "${var.project_name}-airflow-access-point-${var.environment}"
  })
}

resource "aws_efs_access_point" "mlflow" {
  file_system_id = aws_efs_file_system.main.id
  
  posix_user {
    gid = 1000
    uid = 1000
  }
  
  root_directory {
    path = "/mlflow"
    creation_info {
      owner_gid   = 1000
      owner_uid   = 1000
      permissions = "0755"
    }
  }
  
  tags = merge(var.tags, {
    Name = "${var.project_name}-mlflow-access-point-${var.environment}"
  })
}

# IAM Role for ECS Task Execution
resource "aws_iam_role" "ecs_task_execution" {
  name = "${var.project_name}-ecs-task-execution-${var.environment}"
  
  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Action = "sts:AssumeRole"
        Effect = "Allow"
        Principal = {
          Service = "ecs-tasks.amazonaws.com"
        }
      }
    ]
  })
  
  tags = var.tags
}

# IAM Role for ECS Task
resource "aws_iam_role" "ecs_task" {
  name = "${var.project_name}-ecs-task-${var.environment}"
  
  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Action = "sts:AssumeRole"
        Effect = "Allow"
        Principal = {
          Service = "ecs-tasks.amazonaws.com"
        }
      }
    ]
  })
  
  tags = var.tags
}

# ECS Task Execution Role Policy Attachment
resource "aws_iam_role_policy_attachment" "ecs_task_execution" {
  role       = aws_iam_role.ecs_task_execution.name
  policy_arn = "arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy"
}

# ECS Task Role Policy for S3 and other AWS services
resource "aws_iam_policy" "ecs_task_policy" {
  name        = "${var.project_name}-ecs-task-policy-${var.environment}"
  description = "IAM policy for ECS tasks to access AWS services"
  
  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Action = [
          "s3:GetObject",
          "s3:PutObject",
          "s3:DeleteObject",
          "s3:ListBucket"
        ]
        Resource = [
          aws_s3_bucket.mlflow_artifacts.arn,
          "${aws_s3_bucket.mlflow_artifacts.arn}/*",
          aws_s3_bucket.data_storage.arn,
          "${aws_s3_bucket.data_storage.arn}/*",
          aws_s3_bucket.monitoring_reports.arn,
          "${aws_s3_bucket.monitoring_reports.arn}/*"
        ]
      },
      {
        Effect = "Allow"
        Action = [
          "kinesis:PutRecord",
          "kinesis:PutRecords",
          "kinesis:GetRecords",
          "kinesis:GetShardIterator",
          "kinesis:DescribeStream",
          "kinesis:ListStreams"
        ]
        Resource = aws_kinesis_stream.taxi_predictions.arn
      },
      {
        Effect = "Allow"
        Action = [
          "logs:CreateLogGroup",
          "logs:CreateLogStream",
          "logs:PutLogEvents"
        ]
        Resource = "arn:aws:logs:*:*:*"
      },
      {
        Effect = "Allow"
        Action = [
          "elasticfilesystem:AccessedViaMountTarget",
          "elasticfilesystem:AccessPointArn"
        ]
        Resource = [
          aws_efs_file_system.main.arn,
          aws_efs_access_point.airflow.arn,
          aws_efs_access_point.mlflow.arn
        ]
      }
    ]
  })
  
  tags = var.tags
}

# Attach ECS Task Policy
resource "aws_iam_role_policy_attachment" "ecs_task_policy" {
  role       = aws_iam_role.ecs_task.name
  policy_arn = aws_iam_policy.ecs_task_policy.arn
}

# ECS Service for Airflow
resource "aws_ecs_service" "airflow" {
  name            = "${var.project_name}-airflow-${var.environment}"
  cluster         = aws_ecs_cluster.main.id
  task_definition = aws_ecs_task_definition.airflow.arn
  desired_count   = 1
  launch_type     = "FARGATE"
  
  network_configuration {
    subnets          = module.vpc.private_subnet_ids
    security_groups  = [module.security_groups.ecs_security_group_id]
    assign_public_ip = false
  }
  
  load_balancer {
    target_group_arn = aws_lb_target_group.frontend.arn
    container_name   = "airflow-webserver"
    container_port   = 8080
  }
  
  # Ensure ALB is created before service
  depends_on = [aws_lb_listener.main]
  
  # Enable service discovery and health checks
  health_check_grace_period_seconds = 300
  
  # Deployment configuration for zero-downtime updates
  
  tags = var.tags
}

# ECS Service for MLflow
resource "aws_ecs_service" "mlflow" {
  name            = "${var.project_name}-mlflow-${var.environment}"
  cluster         = aws_ecs_cluster.main.id
  task_definition = aws_ecs_task_definition.mlflow.arn
  desired_count   = 1
  launch_type     = "FARGATE"
  
  network_configuration {
    subnets          = module.vpc.private_subnet_ids
    security_groups  = [module.security_groups.ecs_security_group_id]
    assign_public_ip = false
  }
  
  load_balancer {
    target_group_arn = aws_lb_target_group.mlflow.arn
    container_name   = "mlflow-server"
    container_port   = 5000
  }
  
  # Ensure ALB is created before service
  depends_on = [aws_lb_listener.main]

  service_registries {
    registry_arn = aws_service_discovery_service.mlflow.arn
  }
  
  # Enable service discovery and health checks
  health_check_grace_period_seconds = 300
  
  # Deployment configuration for zero-downtime updates
  
  tags = var.tags
}

# Application Load Balancer
resource "aws_lb" "main" {
  name               = "${var.project_name}-alb-${var.environment}"
  internal           = false
  load_balancer_type = "application"
  security_groups    = [module.security_groups.alb_security_group_id]
  subnets            = module.vpc.public_subnet_ids
  
  enable_deletion_protection = var.environment == "prod" ? true : false
  
  tags = var.tags
}

# ALB Target Group for Airflow
resource "aws_lb_target_group" "airflow" {
  name     = "mlops-airflow-tg-${var.environment}"
  port     = 8080
  protocol = "HTTP"
  vpc_id   = module.vpc.vpc_id
  target_type = "ip"
  
  health_check {
    enabled             = true
    healthy_threshold   = 2
    interval            = 30
    matcher             = "200"
    path                = "/health"
    port                = "traffic-port"
    protocol            = "HTTP"
    timeout             = 5
    unhealthy_threshold = 2
  }
  
  tags = var.tags
}

# ALB Target Group for MLflow
resource "aws_lb_target_group" "mlflow" {
  name     = "mlops-mlflow-tg-${var.environment}"
  port     = 5000
  protocol = "HTTP"
  vpc_id   = module.vpc.vpc_id
  target_type = "ip"
  
  health_check {
    enabled             = true
    healthy_threshold   = 2
    interval            = 30
    matcher             = "200"
    path                = "/"
    port                = "traffic-port"
    protocol            = "HTTP"
    timeout             = 5
    unhealthy_threshold = 2
  }
  
  tags = var.tags
}

# ALB Listener
resource "aws_lb_listener" "main" {
  load_balancer_arn = aws_lb.main.arn
  port              = "80"
  protocol          = "HTTP"
  
  default_action {
    type             = "forward"
    target_group_arn = aws_lb_target_group.frontend.arn
  }
}

# ALB Listener Rule for MLflow
resource "aws_lb_listener_rule" "mlflow" {
  listener_arn = aws_lb_listener.main.arn
  priority     = 100
  
  action {
    type             = "forward"
    target_group_arn = aws_lb_target_group.mlflow.arn
  }
  
  condition {
    path_pattern {
      values = ["/mlflow*"]
    }
  }
}

# Data source for existing Route 53 hosted zone (optional, for custom domain)
data "aws_route53_zone" "main" {
  count = var.create_route53_records && var.domain_name != "" ? 1 : 0
  
  name         = var.domain_name
  private_zone = false
}

# Route 53 A record for Airflow subdomain (optional)
resource "aws_route53_record" "airflow" {
  count = var.create_route53_records && var.domain_name != "" ? 1 : 0
  
  zone_id = data.aws_route53_zone.main[0].zone_id
  name    = "${var.airflow_subdomain}.${var.domain_name}"
  type    = "A"
  
  alias {
    name                   = aws_lb.main.dns_name
    zone_id                = aws_lb.main.zone_id
    evaluate_target_health = true
  }
}

# Route 53 A record for MLflow subdomain (optional)
resource "aws_route53_record" "mlflow" {
  count = var.create_route53_records && var.domain_name != "" ? 1 : 0
  
  zone_id = data.aws_route53_zone.main[0].zone_id
  name    = "${var.mlflow_subdomain}.${var.domain_name}"
  type    = "A"
  
  alias {
    name                   = aws_lb.main.dns_name
    zone_id                = aws_lb.main.zone_id
    evaluate_target_health = true
  }
}

# Service Discovery Namespace
resource "aws_service_discovery_private_dns_namespace" "main" {
  name        = "${var.project_name}-${var.environment}.local"
  description = "Private DNS namespace for service discovery"
  vpc         = module.vpc.vpc_id

  tags = var.tags
}

# Service Discovery Service for MLflow
resource "aws_service_discovery_service" "mlflow" {
  name = "mlflow"

  dns_config {
    namespace_id = aws_service_discovery_private_dns_namespace.main.id

    dns_records {
      ttl  = 10
      type = "A"
    }

    routing_policy = "MULTIVALUE"
  }

  health_check_custom_config {
    failure_threshold = 1
  }

  tags = var.tags
}

# One-time task to initialize Airflow database
resource "aws_ecs_task_definition" "airflow_db_init" {
  family                   = "${var.project_name}-airflow-db-init-${var.environment}"
  network_mode             = "awsvpc"
  requires_compatibilities = ["FARGATE"]
  cpu                      = "512"
  memory                   = "1024"
  execution_role_arn       = aws_iam_role.ecs_task_execution.arn
  task_role_arn           = aws_iam_role.ecs_task.arn

  container_definitions = jsonencode([
    {
      name      = "airflow-db-init"
      image     = "${module.ecr.airflow_repository_url}:latest"
      command   = ["db", "migrate"]
      essential = true
      
      environment = [
        {
          name  = "AIRFLOW__DATABASE__SQL_ALCHEMY_CONN"
          value = "postgresql+psycopg2://${var.airflow_db_username}:${var.airflow_db_password}@${aws_db_instance.airflow.endpoint}/${var.airflow_db_name}"
        },
        {
          name  = "AIRFLOW__CORE__FERNET_KEY"
          value = var.airflow_fernet_key
        }
      ]
      
      logConfiguration = {
        logDriver = "awslogs"
        options = {
          "awslogs-group"         = aws_cloudwatch_log_group.airflow.name
          "awslogs-region"        = var.aws_region
          "awslogs-stream-prefix" = "airflow-db-init"
        }
      }
    }
  ])

  tags = var.tags
}

# One-time task to create Airflow admin user
# resource "aws_ecs_task_definition" "airflow_create_user" {
#   family                   = "${var.project_name}-airflow-create-user-${var.environment}"
#   network_mode             = "awsvpc"
#   requires_compatibilities = ["FARGATE"]
#   cpu                      = "512"
#   memory                   = "1024"
#   execution_role_arn       = aws_iam_role.ecs_task_execution.arn
#   task_role_arn           = aws_iam_role.ecs_task.arn
# 
#   container_definitions = jsonencode([
#     {
#       name      = "airflow-create-user"
#       image     = "${module.ecr.airflow_repository_url}:latest"
#       command   = ["airflow", "users", "create", "-u", "admin", "-f", "Admin", "-l", "Admin", "-r", "Admin", "-e", "admin@example.com", "-p", "admin123"]
#       essential = true
#       
#       environment = [
#         {
#           name  = "AIRFLOW__DATABASE__SQL_ALCHEMY_CONN"
#           value = "postgresql+psycopg2://${var.airflow_db_username}:${var.airflow_db_password}@${aws_db_instance.airflow.endpoint}/${var.airflow_db_name}"
#         },
#         {
#           name  = "AIRFLOW__CORE__FERNET_KEY"
#           value = var.airflow_fernet_key
#         }
#       ]
#       
#       logConfiguration = {
#         logDriver = "awslogs"
#         options = {
#           "awslogs-group"         = aws_cloudwatch_log_group.airflow.name
#           "awslogs-region"        = var.aws_region
#           "awslogs-stream-prefix" = "airflow-create-user"
#         }
#       }
#     }
#   ])
# 
#   tags = var.tags
# }
